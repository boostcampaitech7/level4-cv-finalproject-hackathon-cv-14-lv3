from fastapi import FastAPI, WebSocket, WebSocketDisconnect,HTTPException
from fastapi.middleware.cors import CORSMiddleware
import httpx
from datetime import datetime
import pandas as pd
import sqlalchemy
import re
from dotenv import load_dotenv
import os
import requests
import json
from openai import OpenAI
from supabase import create_client
from supabase.lib.client_options import ClientOptions
import time
import sqlite3
from pydantic import BaseModel
from typing import List
import random
import numpy as np
import matplotlib.pyplot as plt
from tqdm.auto import tqdm
from sklearn.preprocessing import LabelEncoder

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
print(device)

CFG = {
    'TRAIN_WINDOW_SIZE':60,
    'PREDICT_SIZE':1,
    'EPOCHS':50,
    'LEARNING_RATE':1e-4,
    'BATCH_SIZE':1024,
    'SEED': 42
}

def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True

seed_everything(CFG['SEED'])

train_cat = pd.read_csv('data/train.csv').drop(columns=['ID'])
train_num = pd.read_csv('data/train.csv').drop(columns=['ID'])

train_cat = train_cat.iloc[:,:4]
train_num = train_num.iloc[:, 4:-41]

train_data = pd.concat([train_cat, train_num], axis=1)

out_train = pd.read_csv('data/train.csv')
out_train = out_train.iloc[:, -7:]

train_data = pd.concat([train_data, out_train], axis=1)

# ìˆ«ìí˜• ë³€ìˆ˜ë“¤ì˜ min-max scalingì„ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤. real_train
numeric_cols = train_data.columns[4:]

# ê° columnì˜ min ë° max ê³„ì‚°
min_values = train_data[numeric_cols].min(axis = 1)
max_values = train_data[numeric_cols].max(axis = 1)

# ê° í–‰ì˜ ë²”ìœ„(max-min)ë¥¼ ê³„ì‚°í•˜ê³ , ë²”ìœ„ê°€ 0ì¸ ê²½ìš° 1ë¡œ ëŒ€ì²´
ranges = max_values - min_values
ranges[ranges == 0] = 1

# min-max scaling ìˆ˜í–‰
train_data[numeric_cols] = (train_data[numeric_cols].subtract(min_values, axis = 0)).div(ranges, axis = 0)

# maxì™€ min ê°’ì„ dictionary í˜•íƒœë¡œ ì €ì¥
scale_min_dict = min_values.to_dict()
scale_max_dict = max_values.to_dict()

# 1. ë²”ì£¼í˜• ë³€ìˆ˜ ë ˆì´ë¸” ì¸ì½”ë”©
label_encoders = {}  # ê° ì»¬ëŸ¼ë³„ë¡œ LabelEncoderë¥¼ ì €ì¥
categorical_columns = ['Main', 'Sub1', 'Sub2', 'Sub3']

for col in categorical_columns:
    le = LabelEncoder()
    train_data[col] = le.fit_transform(train_data[col]).astype(int)
    label_encoders[col] = le

# 2. ì„ë² ë”© ë ˆì´ì–´ ìƒì„±
class CategoricalEmbedding(nn.Module):
    def __init__(self, input_sizes, embedding_dims):
        super(CategoricalEmbedding, self).__init__()

        # ê° ë²”ì£¼í˜• ë³€ìˆ˜ì— ëŒ€í•œ ì„ë² ë”© ë ˆì´ì–´ë¥¼ ìƒì„±
        self.embeddings = nn.ModuleList([
            nn.Embedding(input_size, dim) for input_size, dim in zip(input_sizes, embedding_dims)
        ])

    def forward(self, x):
        # x: [batch_size, num_categorical_features]
        embedded = [embedding(x[:, i]) for i, embedding in enumerate(self.embeddings)]
        return torch.cat(embedded, dim=1)  # ì—°ê²°ëœ ì„ë² ë”© ë²¡í„° ë°˜í™˜

# ê° ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ìµœëŒ€ê°’ (ë ˆì´ë¸” ì¸ì½”ë”©ëœ ê°’) + 1ì„ êµ¬í•¨
input_sizes = [train_data[col].max() + 1 for col in categorical_columns]

# ì„ë² ë”© ì°¨ì› ì„¤ì •
embedding_dims = [int(np.sqrt(size) // 2) for size in input_sizes]

model = CategoricalEmbedding(input_sizes, embedding_dims)

# ëª¨ë“  í–‰ì— ëŒ€í•œ ë²”ì£¼í˜• ë°ì´í„°ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜
all_data_tensor = torch.tensor(train_data[categorical_columns].values, dtype = torch.long)

# ì„ë² ë”© ëª¨ë¸ì— í…ì„œë¥¼ ì…ë ¥í•˜ì—¬ ì„ë² ë”©ëœ ê°’ì„ ì–»ìŒ
with torch.no_grad():
    all_embedded_values = model(all_data_tensor)

# ì„ë² ë”©ëœ í…ì„œë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
all_embedded_np = all_embedded_values.numpy()

# ì„ë² ë”©ëœ ê°’ì„ ì €ì¥í•  ì„ì‹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
embedded_df = pd.DataFrame()

start_idx = 0
# ê° ë²”ì£¼í˜• ë³€ìˆ˜ì— ëŒ€í•œ ì„ë² ë”©ëœ ê°’ì„ ìƒˆë¡œìš´ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€
for i, col in enumerate(categorical_columns):
    col_names = [f"{col}_{j}" for j in range(embedding_dims[i])]
    for idx, name in enumerate(col_names):
        embedded_df[name] = all_embedded_np[:, start_idx + idx]
    start_idx += embedding_dims[i]

# ë ˆì´ë¸” ì¸ì½”ë”©ëœ ì»¬ëŸ¼ ì œê±°
train_data.drop(columns=categorical_columns, inplace = True)

# ì„ë² ë”©ëœ ë°ì´í„°ë¥¼ ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì˜ ì• ë¶€ë¶„ì— ì¶”ê°€
train_data = pd.concat([embedded_df, train_data], axis = 1)

# ê²°ê³¼ í™•ì¸
train_data.head()

def make_predict_data(data, train_size = CFG['TRAIN_WINDOW_SIZE']):
    num_rows = len(data)

    input_data = np.empty((num_rows, train_size, len(data.iloc[0, :33]) + 1))

    for i in tqdm(range(num_rows)):
        encode_info = np.array(data.iloc[i, :33])
        sales_data = np.array(data.iloc[i, -train_size:])

        window = sales_data[-train_size : ]
        temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))
        input_data[i] = temp_data

    return input_data

test_input = make_predict_data(train_data)

# Custom Dataset
class CustomDataset(Dataset):
    def __init__(self, X, Y):
        self.X = X
        self.Y = Y

    def __getitem__(self, index):
        if self.Y is not None:
            return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])
        return torch.Tensor(self.X[index])

    def __len__(self):
        return len(self.X)

test_dataset = CustomDataset(test_input, None)
test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle = False, num_workers = 0)

def inference(model, test_loader, device):
    predictions = []

    with torch.no_grad():
        for X in tqdm(iter(test_loader)):
            X = X.to(device)

            output = model(X)

            # ëª¨ë¸ ì¶œë ¥ì¸ outputì„ CPUë¡œ ì´ë™í•˜ê³  numpy ë°°ì—´ë¡œ ë³€í™˜
            output = output.cpu().numpy()

            predictions.extend(output)

    return np.array(predictions)

class Mish(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return x * torch.tanh(nn.functional.softplus(x))

class StackedLSTMModel(nn.Module):
    def __init__(self, input_size = 34, hidden_size = 1024, output_size = CFG['PREDICT_SIZE'], num_layers = 3, dropout = 0.5):
        super(StackedLSTMModel, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTM ë ˆì´ì–´ ë‚´ë¶€ì— dropout ì ìš©
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, dropout = (0 if num_layers == 1 else dropout), batch_first = True)

        self.fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size//2),
            Mish(),
            nn.Linear(hidden_size//2, output_size)
        )

        self.actv = Mish()

    def forward(self, x):
        # x shape: (B, TRAIN_WINDOW_SIZE, 5)
        batch_size = x.size(0)
        hidden = self.init_hidden(batch_size, x.device)

        # LSTM layers
        x, hidden = self.lstm(x, hidden)

        # Only use the last output sequence
        last_output = x[:, -1, :]

        # Fully connected layer
        output = self.actv(self.fc(last_output))

        return output.squeeze(1)

    def init_hidden(self, batch_size, device):
        # Initialize hidden state and cell state
        return (torch.zeros(self.num_layers, batch_size, self.hidden_size, device = device),
                torch.zeros(self.num_layers, batch_size, self.hidden_size, device = device))

# 1) FastAPI ì•± ìƒì„±
app = FastAPI()

# WebSocket ì—°ê²° ê´€ë¦¬ 
active_connections = []

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
UPSTAGE_API_KEY = os.getenv('UPSTAGE_API_KEY')
SUPABASE_URL = os.getenv('SUPABASE_URL')
SUPABASE_KEY = os.getenv('SUPABASE_KEY')

# Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
options = ClientOptions(postgrest_client_timeout=600)  # íƒ€ì„ì•„ì›ƒì„ 600ì´ˆë¡œ ì„¤ì •
supabase = create_client(SUPABASE_URL, SUPABASE_KEY, options)

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(
    api_key=UPSTAGE_API_KEY,
    base_url=UPSTAGE_API_BASE_URL
)

# ===== ë°ì´í„° ë¡œë”© =====
# Supabase ì—°ê²° ì¬ì„¤ì • í•¨ìˆ˜
def reconnect_supabase():
    global supabase
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    print("Reconnected to Supabase.")

# Supabaseì—ì„œ ëª¨ë“  ë ˆì½”ë“œë¥¼ í˜ì´ì§€ë„¤ì´ì…˜(Chunk) ë°©ì‹ìœ¼ë¡œ ê°€ì ¸ì˜¤ëŠ” ê³µí†µ í•¨ìˆ˜
def load_all_data(table_name):
    all_data = []
    chunk_size = 100000
    start = 0

    while True:
        response = (
            supabase
            .table(table_name)
            .select("*")
            .range(start, start + chunk_size - 1)
            .execute()
        )
        fetched_data = response.data

        # ê° ì‚¬ì´í´ì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„° ê°œìˆ˜ ì¶œë ¥
        print(f"Fetched {len(fetched_data)} rows from {table_name} (range: {start}~{start + chunk_size - 1})")

        if not fetched_data:
            # ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
            break

        all_data.extend(fetched_data)
        start += chunk_size

    return all_data

# daily_sales_data í…Œì´ë¸”ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ì½ì–´ì™€ DataFrameìœ¼ë¡œ ë³€í™˜
def load_sales_data():
    try:
        # ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©
        db_path = os.path.join(os.path.dirname(__file__), "../database/database.db")
        conn = sqlite3.connect(db_path)

        # idë¥¼ ì •ìˆ˜ë¡œ ê°€ì ¸ì˜¤ë„ë¡ ì¿¼ë¦¬ ìˆ˜ì •
        df = pd.read_sql_query("SELECT CAST(id AS INTEGER) as id, date, value FROM daily_sales_data", conn)
        conn.close()

        # ì—´ ì´ë¦„ì„ ì†Œë¬¸ìë¡œ ë³€í™˜
        df.columns = df.columns.str.lower()

        # date ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ë©´ datetimeìœ¼ë¡œ ë³€í™˜
        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"], errors="coerce")

        return df
    except Exception as e:
        print(f"Error loading sales data: {str(e)}")
        print(f"Current working directory: {os.getcwd()}")
        print(f"Database path: {db_path}")
        raise

# daily_data í…Œì´ë¸”ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ì½ì–´ì™€ DataFrameìœ¼ë¡œ ë³€í™˜
def load_quantity_data():
    try:
        # ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©
        db_path = os.path.join(os.path.dirname(__file__), "../database/database.db")
        conn = sqlite3.connect(db_path)

        # idë¥¼ ì •ìˆ˜ë¡œ ê°€ì ¸ì˜¤ë„ë¡ ì¿¼ë¦¬ ìˆ˜ì •
        df = pd.read_sql_query("SELECT CAST(id AS INTEGER) as id, date, value FROM daily_data", conn)
        conn.close()

        # ì—´ ì´ë¦„ì„ ì†Œë¬¸ìë¡œ ë³€í™˜
        df.columns = df.columns.str.lower()

        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"], errors="coerce")
        return df
    except Exception as e:
        print(f"Error loading quantity data: {str(e)}")
        print(f"Current working directory: {os.getcwd()}")
        print(f"Database path: {db_path}")
        raise

# product_inventory í…Œì´ë¸”ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ì½ì–´ì™€ DataFrameìœ¼ë¡œ ë³€í™˜
def load_inventory_data():
    all_data = load_all_data("product_inventory")
    df = pd.DataFrame(all_data)
    return df

# trend_product í…Œì´ë¸”ì˜ ë°ì´í„°ë¥¼ ì½ì–´ì˜¤ëŠ” í•¨ìˆ˜
def load_trend_data():
    try:
        response = supabase.table('trend_product') \
            .select('product_name, rank, category, id') \
            .eq('rank', 1) \
            .execute()

        df = pd.DataFrame(response.data)
        return df
    except Exception as e:
        print(f"Error loading trend data: {str(e)}")
        raise

# ë°ì´í„° ë¡œë“œ
df_sales = load_sales_data()
reconnect_supabase()
inventory_df = load_inventory_data()
reconnect_supabase()
df_quantity = load_quantity_data()
reconnect_supabase()
trend_df = load_trend_data()  # íŠ¸ë Œë“œ ë°ì´í„° ë¡œë“œ ì¶”ê°€

# ===== ì»¬ëŸ¼ëª… ë³€ê²½ =====
df_sales = df_sales.rename(columns={'value': 'ë§¤ì¶œì•¡'})
df_quantity = df_quantity.rename(columns={'value': 'íŒë§¤ìˆ˜ëŸ‰'})
inventory_df = inventory_df.rename(columns={'value': 'ì¬ê³ ìˆ˜ëŸ‰'})

# ë¬¸ìì—´ ë³€í™˜
df_sales['date'] = df_sales['date'].astype(str)
df_quantity['date'] = df_quantity['date'].astype(str)

# ë§Œì•½ df_salesì— 'ëŒ€ë¶„ë¥˜' í˜¹ì€ 'ì†Œë¶„ë¥˜' ì»¬ëŸ¼ì´ ì—†ë‹¤ë©´ product_info ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ ë³‘í•©
if 'ì†Œë¶„ë¥˜' not in df_sales.columns or 'ëŒ€ë¶„ë¥˜' not in df_sales.columns:
    # product_info í…Œì´ë¸”ì—ëŠ” id, main, sub1, sub2, sub3ê°€ ìˆìŒ (ì—¬ê¸°ì„œëŠ” sub1: ëŒ€ë¶„ë¥˜, sub3: ì†Œë¶„ë¥˜ë¡œ ì‚¬ìš©)
    product_info_response = supabase.from_('product_info').select("id, sub1, sub3").execute()
    df_product_info = pd.DataFrame(product_info_response.data)
    # í•„ìš”í•œ ì»¬ëŸ¼ëª…ì„ ë³€ê²½í•©ë‹ˆë‹¤.
    df_product_info = df_product_info.rename(columns={"sub1": "ëŒ€ë¶„ë¥˜", "sub3": "ì†Œë¶„ë¥˜"})

    # id ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
    df_sales['id'] = df_sales['id'].astype(str)
    df_product_info['id'] = df_product_info['id'].astype(str)

    # df_salesì— product_infoë¥¼ idë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•© (left join)
    df_sales = df_sales.merge(df_product_info[["id", "ëŒ€ë¶„ë¥˜", "ì†Œë¶„ë¥˜"]], on="id", how="left")

# ===== Dash ì½”ë“œì—ì„œ í•˜ë˜ ì „ì²˜ë¦¬ ë¡œì§ =====

data = df_sales.copy()
data_quantity = df_quantity.copy()

data['ë‚ ì§œ'] = pd.to_datetime(data['date'])
data_quantity['ë‚ ì§œ'] = pd.to_datetime(data_quantity['date'])

# ì¼ê°„ í•©ê³„
daily_df = data.groupby('ë‚ ì§œ', as_index=False)['ë§¤ì¶œì•¡'].sum()
daily_df = daily_df.rename(columns={'ë§¤ì¶œì•¡': 'ê°’'})

# ì£¼ê°„ ì§‘ê³„ (ë‚ ì§œë¥¼ ì£¼ ì‹œì‘ì¼ë¡œ ë³€í™˜í•˜ì—¬ ê·¸ë£¹í™”)
daily_df['ì£¼ê°„'] = daily_df['ë‚ ì§œ'].dt.to_period('W').apply(lambda r: r.start_time)
weekly_data = daily_df.groupby('ì£¼ê°„', as_index=False)['ê°’'].sum()

# ì›”ê°„ ì§‘ê³„ (ë‚ ì§œë¥¼ ì›” ì‹œì‘ì¼ë¡œ ë³€í™˜í•˜ì—¬ ê·¸ë£¹í™”)
daily_df['ì›”ê°„'] = daily_df['ë‚ ì§œ'].dt.to_period('M').apply(lambda r: r.start_time)
monthly_sum_df = daily_df.groupby('ì›”ê°„', as_index=False)['ê°’'].sum()

# ìµœê·¼ 12ê°œì›” í•©ê³„ë¥¼ "ì—°ê°„ ë§¤ì¶œ"ë¡œ ê°€ì •
recent_12_months = monthly_sum_df.tail(12)
annual_sales = recent_12_months['ê°’'].sum()

# KPI ê³„ì‚°
daily_avg = daily_df['ê°’'].mean() if not daily_df.empty else 0
weekly_avg = weekly_data['ê°’'].mean() if not weekly_data.empty else 0
monthly_avg = monthly_sum_df['ê°’'].mean() if not monthly_sum_df.empty else 0
last_daily = daily_df['ê°’'].iloc[-1] if not daily_df.empty else 0
last_weekly = weekly_data['ê°’'].iloc[-1] if not weekly_data.empty else 0
last_monthly = monthly_sum_df['ê°’'].iloc[-1] if not monthly_sum_df.empty else 0

# ì›”ê°„ ë³€í™”ìœ¨ ê³„ì‚°
if len(monthly_sum_df) >= 2:
    lm_sales = monthly_sum_df['ê°’'].iloc[-1]
    slm_sales = monthly_sum_df['ê°’'].iloc[-2]
    monthly_change = ((lm_sales - slm_sales) / slm_sales) * 100 if slm_sales != 0 else 0
else:
    monthly_change = 0

# ì¹´í…Œê³ ë¦¬ë³„(ëŒ€ë¶„ë¥˜) ë§¤ì¶œ
df_category = df_sales.groupby('ëŒ€ë¶„ë¥˜', as_index=False)['ë§¤ì¶œì•¡'].sum()

# ì¬ê³ ìˆ˜ëŸ‰ ë° ì¼ì¼íŒë§¤ìˆ˜ëŸ‰ ì²˜ë¦¬
data_quantity['ë‚ ì§œ'] = pd.to_datetime(data_quantity['date'])
last_date = data_quantity['ë‚ ì§œ'].max()
if last_date:
    daily_sales_quantity_last = data_quantity[data_quantity['ë‚ ì§œ'] == last_date][['id', 'íŒë§¤ìˆ˜ëŸ‰']]
    daily_sales_quantity_last = daily_sales_quantity_last.rename(columns={'íŒë§¤ìˆ˜ëŸ‰': 'ì¼íŒë§¤ìˆ˜ëŸ‰'})
else:
    daily_sales_quantity_last = pd.DataFrame(columns=['id', 'ì¼íŒë§¤ìˆ˜ëŸ‰'])

merged_df = pd.merge(inventory_df, daily_sales_quantity_last, on='id', how='left')
merged_df['ì¼íŒë§¤ìˆ˜ëŸ‰'] = merged_df['ì¼íŒë§¤ìˆ˜ëŸ‰'].fillna(0)
merged_df['ë‚¨ì€ ì¬ê³ '] = merged_df['ì¬ê³ ìˆ˜ëŸ‰'] - merged_df['ì¼íŒë§¤ìˆ˜ëŸ‰']
low_stock_df = merged_df[(merged_df['ë‚¨ì€ ì¬ê³ '] >= 0) & (merged_df['ë‚¨ì€ ì¬ê³ '] <= 30)]

# ë§¤ì¶œ ìƒìŠ¹í­(ì†Œë¶„ë¥˜)
# ë§Œì•½ ì†Œë¶„ë¥˜ ê°’ì´ ì—†ëŠ” ê²½ìš° "ê¸°íƒ€" ì²˜ë¦¬
data["ì†Œë¶„ë¥˜"] = data["ì†Œë¶„ë¥˜"].fillna('ê¸°íƒ€')
pivot_subcat = data.groupby(['ì†Œë¶„ë¥˜', 'ë‚ ì§œ'])['ë§¤ì¶œì•¡'].sum().unstack().fillna(0)
all_dates = sorted(pivot_subcat.columns)
if len(all_dates) >= 2:
    last_date_val, second_last_date_val = all_dates[-1], all_dates[-2]
    last_sum = pivot_subcat[last_date_val].sum()
    second_sum = pivot_subcat[second_last_date_val].sum()
    rise_rate = ((last_sum - second_sum) / second_sum) * 100 if second_sum != 0 else 0
else:
    rise_rate = 0

subcat_list = pivot_subcat.sum(axis=1).sort_values(ascending=False).head(10).index.tolist()

# ---------- (ì¶”ê°€) ìƒ/í•˜ìœ„ 10ê°œ ê³„ì‚° ----------
monthly_data = (
    data.assign(ì›”=lambda df: df['ë‚ ì§œ'].dt.to_period('M').astype(str))
    .groupby(['id', 'ì›”'], as_index=False)['ë§¤ì¶œì•¡'].sum()
)
result = monthly_data.pivot(index='id', columns='ì›”', values='ë§¤ì¶œì•¡').reset_index()
result['id'] = result['id'].astype(str)
if len(result.columns) > 1:
    last_month_col = result.columns[-1]
else:
    last_month_col = None

reds = ['#D14B4B', '#E22B2B', '#E53A3A', '#F15D5D', '#F67878',
        '#F99A9A', '#FBB6B6', '#FDC8C8', '#FEE0E0', '#FEEAEA']
blues = ['#B0D6F1', '#A5C9E9', '#99BCE1', '#8DB0D9', '#81A4D1',
         '#7498C9', '#688BC1', '#5C7FB9', '#5073B1', '#4567A9']

# ===== ì—”ë“œí¬ì¸íŠ¸ë“¤ =====

@app.get("/")
def read_root():
    return {
        "status": "success",
    }

@app.get("/api/kpis")
def get_kpis():
    # ë„˜íŒŒì´ ì •ìˆ˜/ì‹¤ìˆ˜ -> íŒŒì´ì¬ int/float
    return {
        "annual_sales": int(annual_sales),
        "daily_avg": float(daily_avg),
        "weekly_avg": float(weekly_avg),
        "monthly_avg": float(monthly_avg),
        "last_daily": int(last_daily),
        "last_weekly": int(last_weekly),
        "last_monthly": int(last_monthly),
        "monthly_change": float(monthly_change),
    }

@app.get("/api/daily")
def get_daily_data():
    # 2023ë…„ ì´í›„ ë°ì´í„°ë§Œ í•„í„°ë§
    filtered_daily = daily_df
    return filtered_daily.to_dict(orient="records")

@app.get("/api/weekly")
def get_weekly_data():
    # 2022ë…„ 10ì›” ì´í›„ ë°ì´í„°ë§Œ í•„í„°ë§
    filtered_weekly = weekly_data
    return filtered_weekly.to_dict(orient="records")

@app.get("/api/monthly")
def get_monthly_data():
    return monthly_sum_df.to_dict(orient="records")

@app.get("/api/categorypie")
def get_category_pie():
    # ë§¤ì¶œì•¡ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ 5ê°œë§Œ ì„ íƒ
    top_5_categories = df_category.nlargest(5, 'ë§¤ì¶œì•¡')
    return top_5_categories.to_dict(orient="records")

@app.get("/api/lowstock")
def get_low_stock():
    # merged_dfì™€ low_stock_dfê°€ ì „ì—­ ë³€ìˆ˜ë¡œ ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    global merged_df, low_stock_df

    try:
        # product_info í…Œì´ë¸”ì—ì„œ sub3 ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        response = supabase.table('product_info').select("id", "sub3").execute()
        product_info = pd.DataFrame(response.data)
        product_info = product_info.rename(columns={'id': 'id', 'sub3': 'Sub3'})

        # low_stock_dfì™€ product_info ë³‘í•©
        merged_low_stock = pd.merge(low_stock_df, product_info, on='id', how='left')

        return merged_low_stock.to_dict(orient="records")
    except Exception as e:
        print(f"Error in get_low_stock: {str(e)}")
        return {"error": str(e)}

@app.get("/api/rising-subcategories")
def get_rising_subcategories():
    return {
        "rise_rate": float(rise_rate),
        "subcat_list": list(subcat_list)  # ë„˜íŒŒì´ Index -> list
    }

# (ìˆ˜ì •) íŒë§¤ìˆ˜ëŸ‰ ìƒìœ„ 10ê°œ í’ˆëª© ë°˜í™˜ ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/topbottom")
def get_topbottom():
    try:
        # df_quantityëŠ” ì´ë¯¸ ì „ì—­ ë³€ìˆ˜ë¡œ ë¡œë“œë˜ì–´ ìˆìŒ (ì»¬ëŸ¼: id, date, íŒë§¤ìˆ˜ëŸ‰)
        # ì „ì²´ íŒë§¤ìˆ˜ëŸ‰ì„ ì œí’ˆë³„ë¡œ ì§‘ê³„
        sales_qty_total = df_quantity.groupby('id', as_index=False)['íŒë§¤ìˆ˜ëŸ‰'].sum()

        # íŒë§¤ìˆ˜ëŸ‰ ìƒìœ„ 10ê°œ í’ˆëª© ì„ íƒ
        top_10_df = sales_qty_total.nlargest(10, 'íŒë§¤ìˆ˜ëŸ‰').copy()
        top_10_df['id'] = top_10_df['id'].astype(str)
        top_10_df = top_10_df.rename(columns={'id': 'ID', 'íŒë§¤ìˆ˜ëŸ‰': 'ì´íŒë§¤ìˆ˜ëŸ‰'})

        # Supabaseì—ì„œ product_info ë°ì´í„° (ì˜ˆ: ì œí’ˆëª… ë˜ëŠ” sub3 ì •ë³´ë¥¼ ê°€ì ¸ì˜´)
        response = supabase.table('product_info').select("id, sub3").execute()
        product_info = pd.DataFrame(response.data)
        product_info['id'] = product_info['id'].astype(str)
        product_info = product_info.rename(columns={'id': 'ID', 'sub3': 'Sub3'})

        # ì§‘ê³„ ë°ì´í„°ì™€ product_infoë¥¼ ë³‘í•© (ì œí’ˆëª… ë“± ì¶”ê°€ ì •ë³´ í¬í•¨)
        top_10_df = pd.merge(top_10_df, product_info, on='ID', how='left')

        top_10_list = top_10_df.to_dict('records')
        return {
            "top_10": top_10_list
        }
    except Exception as e:
        print(f"Error in get_topbottom: {str(e)}")
        return {
            "top_10": [],
            "error": str(e)
        }

# ì±—ë´‡ìš© ë°ì´í„° ë¯¸ë¦¬ ì¤€ë¹„
def prepare_chat_data():
    # ì›”ë³„ ë§¤ì¶œ ë°ì´í„°
    monthly_sales_text = "ì›”ë³„ ë§¤ì¶œ ë°ì´í„°:\n" + "\n".join([
        f"{row['ì›”ê°„'].strftime('%Y-%m')}: {row['ê°’']:,}ì›"
        for row in monthly_sum_df.to_dict('records')
    ])

    # ì£¼ê°„ ë§¤ì¶œ ë°ì´í„°
    weekly_sales_text = "ì£¼ê°„ ë§¤ì¶œ ë°ì´í„°:\n" + "\n".join([
        f"{row['ì£¼ê°„'].strftime('%Y-%m-%d')}: {row['ê°’']:,}ì›"
        for row in weekly_data.tail(12).to_dict('records')
    ])

    # ì¼ë³„ ë§¤ì¶œ ë°ì´í„°
    daily_sales_text = "ìµœê·¼ 30ì¼ ì¼ë³„ ë§¤ì¶œ ë°ì´í„°:\n" + "\n".join([
        f"{row['ë‚ ì§œ'].strftime('%Y-%m-%d')}: {row['ê°’']:,}ì›"
        for row in daily_df.tail(30).to_dict('records')
    ])

    # ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ ìƒì„¸
    category_details = df_sales.groupby('ëŒ€ë¶„ë¥˜').agg({
        'ë§¤ì¶œì•¡': ['sum', 'mean', 'count']
    }).reset_index()
    category_details.columns = ['ëŒ€ë¶„ë¥˜', 'ì´ë§¤ì¶œ', 'í‰ê· ë§¤ì¶œ', 'íŒë§¤ê±´ìˆ˜']

    category_text = "ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ ìƒì„¸:\n" + "\n".join([
        f"{row['ëŒ€ë¶„ë¥˜']}: ì´ë§¤ì¶œ {row['ì´ë§¤ì¶œ']:,}ì›, í‰ê·  {row['í‰ê· ë§¤ì¶œ']:,.0f}ì›, {row['íŒë§¤ê±´ìˆ˜']}ê±´"
        for _, row in category_details.iterrows()
    ])

    # ì¬ê³  í˜„í™© ìƒì„¸
    inventory_status = pd.merge(
        inventory_df,
        daily_sales_quantity_last,
        on='id',
        how='left'
    )

    # product_info í…Œì´ë¸”ì—ì„œ ì¹´í…Œê³ ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    product_info_response = supabase.from_('product_info').select("id,main,sub3").execute()
    product_info_df = pd.DataFrame(product_info_response.data)

    # ë°ì´í„° ë³‘í•©
    inventory_status = pd.merge(
        inventory_status,
        product_info_df,
        left_on='id',
        right_on='id',
        how='left'
    )

    inventory_status['ì¼íŒë§¤ìˆ˜ëŸ‰'] = inventory_status['ì¼íŒë§¤ìˆ˜ëŸ‰'].fillna(0)
    inventory_status['ë‚¨ì€ì¬ê³ '] = inventory_status['ì¬ê³ ìˆ˜ëŸ‰'] - inventory_status['ì¼íŒë§¤ìˆ˜ëŸ‰']

    inventory_text = "ì¹´í…Œê³ ë¦¬ë³„ ì¬ê³  í˜„í™©:\n" + "\n".join([
        f"{row['main'] if pd.notna(row['main']) else 'ë¯¸ë¶„ë¥˜'}({row['sub3'] if pd.notna(row['sub3']) else 'ë¯¸ë¶„ë¥˜'}): "
        f"ì´ì¬ê³  {row['ì¬ê³ ìˆ˜ëŸ‰']}ê°œ, ì¼íŒë§¤ëŸ‰ {row['ì¼íŒë§¤ìˆ˜ëŸ‰']}ê°œ, ë‚¨ì€ì¬ê³  {row['ë‚¨ì€ì¬ê³ ']}ê°œ"
        for _, row in inventory_status.iterrows()
    ])

    return {
        "monthly_sales": monthly_sales_text,
        "weekly_sales": weekly_sales_text,
        "daily_sales": daily_sales_text,
        "category_details": category_text,
        "inventory_status": inventory_text
    }

# ì±—ë´‡ìš© ë°ì´í„° ë¯¸ë¦¬ ì¤€ë¹„
def prepare_chat_data():
    # ì›”ë³„ ë§¤ì¶œ ë°ì´í„°
    monthly_sales_text = "ì›”ë³„ ë§¤ì¶œ ë°ì´í„°:\n" + "\n".join([
        f"{row['ì›”ê°„'].strftime('%Y-%m')}: {row['ê°’']:,}ì›"
        for row in monthly_sum_df.to_dict('records')
    ])

    # ì£¼ê°„ ë§¤ì¶œ ë°ì´í„°
    weekly_sales_text = "ì£¼ê°„ ë§¤ì¶œ ë°ì´í„°:\n" + "\n".join([
        f"{row['ì£¼ê°„'].strftime('%Y-%m-%d')}: {row['ê°’']:,}ì›"
        for row in weekly_data.tail(12).to_dict('records')
    ])

    # ì¼ë³„ ë§¤ì¶œ ë°ì´í„°
    daily_sales_text = "ìµœê·¼ 30ì¼ ì¼ë³„ ë§¤ì¶œ ë°ì´í„°:\n" + "\n".join([
        f"{row['ë‚ ì§œ'].strftime('%Y-%m-%d')}: {row['ê°’']:,}ì›"
        for row in daily_df.tail(30).to_dict('records')
    ])

    # ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ ìƒì„¸
    category_details = df_sales.groupby('ëŒ€ë¶„ë¥˜').agg({
        'ë§¤ì¶œì•¡': ['sum', 'mean', 'count']
    }).reset_index()
    category_details.columns = ['ëŒ€ë¶„ë¥˜', 'ì´ë§¤ì¶œ', 'í‰ê· ë§¤ì¶œ', 'íŒë§¤ê±´ìˆ˜']

    category_text = "ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ ìƒì„¸:\n" + "\n".join([
        f"{row['ëŒ€ë¶„ë¥˜']}: ì´ë§¤ì¶œ {row['ì´ë§¤ì¶œ']:,}ì›, í‰ê·  {row['í‰ê· ë§¤ì¶œ']:,.0f}ì›, {row['íŒë§¤ê±´ìˆ˜']}ê±´"
        for _, row in category_details.iterrows()
    ])

    # ì¬ê³  í˜„í™© ìƒì„¸
    inventory_status = pd.merge(
        inventory_df,
        daily_sales_quantity_last,
        on='id',
        how='left'
    )

    # product_info í…Œì´ë¸”ì—ì„œ ì¹´í…Œê³ ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    product_info_response = supabase.from_('product_info').select("id,main,sub3").execute()
    product_info_df = pd.DataFrame(product_info_response.data)

    # ë°ì´í„° ë³‘í•©
    inventory_status = pd.merge(
        inventory_status,
        product_info_df,
        left_on='id',
        right_on='id',
        how='left'
    )

    inventory_status['ì¼íŒë§¤ìˆ˜ëŸ‰'] = inventory_status['ì¼íŒë§¤ìˆ˜ëŸ‰'].fillna(0)
    inventory_status['ë‚¨ì€ì¬ê³ '] = inventory_status['ì¬ê³ ìˆ˜ëŸ‰'] - inventory_status['ì¼íŒë§¤ìˆ˜ëŸ‰']

    inventory_text = "ì¹´í…Œê³ ë¦¬ë³„ ì¬ê³  í˜„í™©:\n" + "\n".join([
        f"{row['main'] if pd.notna(row['main']) else 'ë¯¸ë¶„ë¥˜'}({row['sub3'] if pd.notna(row['sub3']) else 'ë¯¸ë¶„ë¥˜'}): "
        f"ì´ì¬ê³  {row['ì¬ê³ ìˆ˜ëŸ‰']}ê°œ, ì¼íŒë§¤ëŸ‰ {row['ì¼íŒë§¤ìˆ˜ëŸ‰']}ê°œ, ë‚¨ì€ì¬ê³  {row['ë‚¨ì€ì¬ê³ ']}ê°œ"
        for _, row in inventory_status.iterrows()
    ])

    return {
        "monthly_sales": monthly_sales_text,
        "weekly_sales": weekly_sales_text,
        "daily_sales": daily_sales_text,
        "category_details": category_text,
        "inventory_status": inventory_text
    }

# ë°ì´í„° ë¯¸ë¦¬ ì¤€ë¹„
CHAT_DATA = prepare_chat_data()

@app.post("/api/chat")
async def chat_with_solar(message: dict):
    try:
        # product_info í…Œì´ë¸”ì—ì„œ sub3 ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        response = supabase.table('product_info').select("id, sub3").execute()
        product_info = pd.DataFrame(response.data)
        product_info = product_info.rename(columns={'id': 'id', 'sub3': 'Sub3'})

        # low_stock_dfì™€ product_info ë³‘í•©
        merged_low_stock = pd.merge(low_stock_df, product_info, on='id', how='left')
        total_low_stock = len(merged_low_stock)

        if total_low_stock > 0:
            low_stock_list = "\n".join([
                f"- {row['Sub3'] if pd.notna(row['Sub3']) else 'ë¯¸ë¶„ë¥˜'}: {row['ë‚¨ì€ ì¬ê³ ']}ê°œ"
                for _, row in merged_low_stock.iterrows()
            ])
        else:
            low_stock_list = "í˜„ì¬ ì¬ê³  ë¶€ì¡± ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤."

        system_message = f"""
        ë‹¹ì‹ ì€ íŒë§¤ ë°ì´í„° ë¶„ì„ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

        1. ë§¤ì¶œì´ë‚˜ ì¬ê³  ê´€ë ¨ ì§ˆë¬¸ì´ ëª¨í˜¸í•œ ê²½ìš°:
        - "ì–´ë–¤ ê¸°ê°„ì˜ ë§¤ì¶œì— ëŒ€í•´ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”? ì¼ê°„, ì£¼ê°„, ì›”ê°„ ë“± êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì‹œë©´ ìì„¸íˆ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
        - "ì–´ë–¤ ì œí’ˆì˜ ì¬ê³ ë¥¼ í™•ì¸í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? íŠ¹ì • ì¹´í…Œê³ ë¦¬ë‚˜ ì œí’ˆì„ ë§ì”€í•´ ì£¼ì‹œë©´ ì •í™•í•œ ì •ë³´ë¥¼ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤."

        2. ë§¤ì¶œì´ë‚˜ ì¬ê³ ì™€ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì¸ ê²½ìš°:
        - "ì£„ì†¡í•©ë‹ˆë‹¤. ì €ëŠ” ë§¤ì¶œê³¼ ì¬ê³  ê´€ë ¨ ë¬¸ì˜ë§Œ ë‹µë³€ ê°€ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."

        3. ë§¤ì¶œì´ë‚˜ ì¬ê³ ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì¸ ê²½ìš°ì—ë§Œ ì•„ë˜ ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”:

        === ë§¤ì¶œ í˜„í™© ===
        - ì—°ê°„ ë§¤ì¶œ: {int(annual_sales):,}ì›
        - ì¼í‰ê·  ë§¤ì¶œ: {float(daily_avg):,.0f}ì›
        - ì£¼ê°„ í‰ê·  ë§¤ì¶œ: {float(weekly_avg):,.0f}ì›
        - ì›”ê°„ í‰ê·  ë§¤ì¶œ: {float(monthly_avg):,.0f}ì›
        - ìµœê·¼ ì¼ì¼ ë§¤ì¶œ: {int(last_daily):,}ì›
        - ìµœê·¼ ì£¼ê°„ ë§¤ì¶œ: {int(last_weekly):,}ì›
        - ìµœê·¼ ì›”ê°„ ë§¤ì¶œ: {int(last_monthly):,}ì›
        - ì „ì›” ëŒ€ë¹„ ë³€í™”ìœ¨: {float(monthly_change):.1f}%

        === ì¹´í…Œê³ ë¦¬ ë¶„ì„ ===
        - ë§¤ì¶œ ìƒìŠ¹ë¥ : {float(rise_rate):.1f}%
        - ì£¼ìš” ì„±ì¥ ì¹´í…Œê³ ë¦¬(ì†Œë¶„ë¥˜): {', '.join(subcat_list[:5])}

        === ì¬ê³  í˜„í™© ===
        - ì¬ê³  ë¶€ì¡± ìƒí’ˆ ìˆ˜: {total_low_stock}ê°œ
        - ì¬ê³  ë¶€ì¡± ìƒí’ˆ ëª©ë¡:
        {low_stock_list}

        ë‹µë³€ ì‹œ ì£¼ì˜ì‚¬í•­:
        1. ë§¤ì¶œ/ì¬ê³  ê´€ë ¨ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì—ë§Œ ê´€ë ¨ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€
        2. ë§¤ì¶œ/ì¬ê³  ê´€ë ¨ ëª¨í˜¸í•œ ì§ˆë¬¸ì—ëŠ” ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ ìš”ì²­
        3. ë§¤ì¶œ/ì¬ê³ ì™€ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì—ëŠ” ë§¤ì¶œ/ì¬ê³  ê´€ë ¨ ë¬¸ì˜ë§Œ ê°€ëŠ¥í•˜ë‹¤ê³  ì•ˆë‚´
        4. ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì–´ì¡° ìœ ì§€
        5. ë¶ˆí•„ìš”í•œ ë°ì´í„°ëŠ” ì œì™¸í•˜ê³  ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë§Œ ì œê³µ
        6. ë‹µë³€í•  ë•Œ ë¶ˆë¦¿ í¬ì¸íŠ¸(-) ì‚¬ìš©í•˜ì§€ ë§ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ í‘œí˜„
        """

        response = client.chat.completions.create(
            model="solar-pro",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": message["content"]}
            ],
            temperature=0.0,
            stream=False,
            response_format={"type": "text/html"}  # HTML í˜•ì‹ ì‘ë‹µ ìš”ì²­
        )

        if hasattr(response, 'error'):
            return {
                "response": f"API ì˜¤ë¥˜: {response.error}",
                "status": "error",
                "error": str(response.error)
            }

        return {
            "response": response.choices[0].message.content,
            "status": "success"
        }

    except Exception as e:
        print(f"Error details: {e!s}")
        return {
            "response": f"ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e!s}",
            "status": "error",
            "error": str(e)
        }

@app.get("/api/trend-categories")
async def get_trend_categories():
    try:
        # ë‹¨ìˆœíˆ trending_product í…Œì´ë¸”ì—ì„œ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì¡°íšŒ
        response = supabase.table('trend_product')\
            .select('category')\
            .execute()
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        categories = sorted(list(set([item['category'] for item in response.data])))
        
        return {
            "status": "success",
            "categories": categories
        }

    except Exception as e:
        print(f"Error in trend-chat: {e!s}")
        return {
            "status": "error",
            "error": f"ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e!s}"
        }

@app.post("/api/trend-chat")
async def chat_with_trend(message: dict):
   try:
       # trend_product í…Œì´ë¸”ì—ì„œ ëª¨ë“  ë°ì´í„° ì¡°íšŒ
       response = supabase.table('trend_product')\
           .select('*')\
           .order('rank')\
           .execute()

       trend_data = response.data

       # íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì •ë¦¬
       categorized_trends = {}
       for item in trend_data:
           category = item['category'].strip()  
           if category not in categorized_trends:
               categorized_trends[category] = []
           categorized_trends[category].append({
               'rank': item['rank'],
               'product_name': item['product_name'],
               'start_date': item['start_date'],
               'end_date': item['end_date']
           })

       # ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ì¹´í…Œê³ ë¦¬ í™•ì¸
       user_category = message["content"].strip()
       
       if user_category in categorized_trends:
           # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ìƒìœ„ 5ê°œ ìˆœìœ„ ì •ë³´ êµ¬ì„±
           trend_info = []
           category_data = sorted(categorized_trends[user_category], key=lambda x: x['rank'])[:5]
           
           # ê¸°ê°„ ì •ë³´ ì¶”ì¶œ
           period = f"{category_data[0]['start_date']} ~ {category_data[0]['end_date']}"
           
           # ìˆœìœ„ ì •ë³´ êµ¬ì„±
           for item in category_data:
               trend_info.append(f"{item['rank']}ìœ„: {item['product_name']}")
           
           trend_text = "\n".join(trend_info)
           
           # ì‹œìŠ¤í…œ ë©”ì‹œì§€ êµ¬ì„±
           system_message = f"""
            You are an AI retail trend analyst specializing in sales improvement.

            Let me show you how to analyze retail trends with multiple examples:

            Example 1:
            Input Data:
            Category: ë””ì§€í„¸/ê°€ì „
            Period: 2024-01-04 ~ 2024-02-04
            Rank 1: ë¬´ì„ ì´ì–´í°
            Rank 2: ê³µê¸°ì²­ì •ê¸°
            Rank 3: ìŠ¤ë§ˆíŠ¸ì›Œì¹˜
            Rank 4: ê°€ìŠµê¸°
            Rank 5: ë¸”ë£¨íˆ¬ìŠ¤ìŠ¤í”¼ì»¤

            Thought Process:
            1. Two out of top 5 are IoT/wearable devices (earphones, smartwatch)
            2. Winter season items (air purifier, humidifier) show seasonal demand
            3. Audio devices (earphones, speaker) indicate strong entertainment needs 
            4. Mix of health (air care) and lifestyle tech shows balanced consumption

            Analysis Result:
            â–¶ [ë””ì§€í„¸/ê°€ì „] ì›”ê°„íŠ¸ë Œë“œ TOP 5
            (2024-01-04 ~ 2024-02-04)
            1ìœ„: ë¬´ì„ ì´ì–´í°
            2ìœ„: ê³µê¸°ì²­ì •ê¸°
            3ìœ„: ìŠ¤ë§ˆíŠ¸ì›Œì¹˜
            4ìœ„: ê°€ìŠµê¸°
            5ìœ„: ë¸”ë£¨íˆ¬ìŠ¤ìŠ¤í”¼ì»¤

            â–¶ íŠ¸ë Œë“œ ë¶„ì„
            ì›¨ì–´ëŸ¬ë¸”/IoT ê¸°ê¸°ì— ëŒ€í•œ ìˆ˜ìš”ê°€ ë†’ê²Œ ë‚˜íƒ€ë‚¬ìœ¼ë©°, ì‹¤ë‚´ ê³µê¸°ì§ˆ ê´€ë¦¬ ê°€ì „ë„ ê°•ì„¸ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ë¬´ì„ ì´ì–´í°ê³¼ ìŠ¤ë§ˆíŠ¸ì›Œì¹˜ì˜ ìƒìœ„ ë­í¬ëŠ” ëª¨ë°”ì¼ ë¼ì´í”„ìŠ¤íƒ€ì¼ì´ ë”ìš± ê°•í™”ë˜ê³  ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ìŒí–¥ê¸°ê¸°ì˜ ê¾¸ì¤€í•œ ìˆ˜ìš”ë„ íŠ¹ì§•ì ì…ë‹ˆë‹¤.

            â–¶ ë‹¤ìŒ ë‹¬ íŠ¸ë Œë“œ ì˜ˆì¸¡
            ì‹ í•™ê¸°ë¥¼ ì•ë‘ê³  ìŠ¤ë§ˆíŠ¸ê¸°ê¸° ìˆ˜ìš”ê°€ ë”ìš± ì¦ê°€í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ë©°, í™˜ì ˆê¸° ëŒ€ë¹„ ê³µê¸°ì§ˆ ê´€ë¦¬ ê°€ì „ì˜ ìˆ˜ìš”ë„ ì§€ì†ë  ê²ƒìœ¼ë¡œ ì „ë§ë©ë‹ˆë‹¤.

            Example 2:
            Input Data:
            Category: ì˜ë¥˜/ì¡í™”
            Period: 2024-01-04 ~ 2024-02-04
            Rank 1: ì—¬ì„± íŒ¨ë”©
            Rank 2: ë‚¨ì„± ìš´ë™í™”
            Rank 3: ì—¬ì„± ë¶€ì¸ 
            Rank 4: ì—¬ì„± ìš´ë™í™”
            Rank 5: ì—¬ì„± í¬ë¡œìŠ¤ë°±

            Thought Process:
            1. Winter apparel dominates with padding and boots
            2. Athletic shoes popular across genders suggests active lifestyle trend
            3. Women's fashion items show stronger presence in rankings
            4. Mix of practical (shoes) and fashion (crossbag) items indicates balanced consumer needs

            Analysis Result:
            â–¶ [ì˜ë¥˜/ì¡í™”] ì›”ê°„íŠ¸ë Œë“œ TOP 5
            (2024-01-04 ~ 2024-02-04)
            1ìœ„: ì—¬ì„± íŒ¨ë”©
            2ìœ„: ë‚¨ì„± ìš´ë™í™”
            3ìœ„: ì—¬ì„± ë¶€ì¸ 
            4ìœ„: ì—¬ì„± ìš´ë™í™”
            5ìœ„: ì—¬ì„± í¬ë¡œìŠ¤ë°±

            â–¶ íŠ¸ë Œë“œ ë¶„ì„
            ë™ì ˆê¸° í•„ìˆ˜ ì•„ì´í…œì¸ íŒ¨ë”©ê³¼ ë¶€ì¸ ê°€ ê°•ì„¸ë¥¼ ë³´ì´ê³  ìˆìœ¼ë©°, íŠ¹íˆ ì—¬ì„± ì˜ë¥˜/ì¡í™”ì˜ ìˆ˜ìš”ê°€ ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤. ìš´ë™í™”ëŠ” ë‚¨ë…€ ëª¨ë‘ì—ê²Œ ì¸ê¸°ê°€ ë†’ì•„ ìºì£¼ì–¼/ìŠ¤í¬í‹°í•œ ìŠ¤íƒ€ì¼ì´ íŠ¸ë Œë“œì„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

            â–¶ ë‹¤ìŒ ë‹¬ íŠ¸ë Œë“œ ì˜ˆì¸¡
            í™˜ì ˆê¸°ë¡œ ì ‘ì–´ë“¤ë©° ê°€ë²¼ìš´ ì•„ìš°í„°ì™€ ìš´ë™í™” ìˆ˜ìš”ê°€ ì§€ì†ë  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ë©°, ë´„ ì‹œì¦Œ ìƒˆë¡œìš´ ìŠ¤íƒ€ì¼ì˜ ì¡í™” ìˆ˜ìš”ê°€ ì¦ê°€í•  ê²ƒìœ¼ë¡œ ì „ë§ë©ë‹ˆë‹¤.

            Now, please analyze the following data using the same thought process:
            Category: {category}
            Period: {period}
            {trend_text}

            Remember to:
            1. First analyze the patterns in the ranking data
            2. Consider seasonal and market factors
            3. Think about consumer behavior and preferences
            4. Make logical predictions based on these factors
            5. Present your analysis in Korean using the same format as the examples
            """

           # Solar ChatAPI ì‘ë‹µ ìƒì„±
           response = client.chat.completions.create(
               model="solar-pro",
               messages=[
                   {"role": "system", "content": system_message},
                   {"role": "user", "content": f"Please analyze the trends for {category} category and provide the response in Korean."}
               ],
               temperature=0.3, 
               stream=False,
               response_format={"type": "text/html"}
           )

           if hasattr(response, 'error'):
               return {
                   "response": f"API ì˜¤ë¥˜: {response.error}",
                   "status": "error",
                   "error": str(response.error)
               }

           return {
               "response": response.choices[0].message.content,
               "status": "success"
           }
       else:
           # ì¹´í…Œê³ ë¦¬ê°€ ì—†ëŠ” ê²½ìš° ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ë°˜í™˜
           available_categories = list(categorized_trends.keys())
           return {
               "response": f"ì£„ì†¡í•©ë‹ˆë‹¤. '{user_category}' ì¹´í…Œê³ ë¦¬ëŠ” í˜„ì¬ íŠ¸ë Œë“œ ì •ë³´ì— ì—†ìŠµë‹ˆë‹¤.\n\nì¡°íšŒ ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬: {', '.join(available_categories)}",
               "status": "success"
           }

   except Exception as e:
       print(f"Error in trend-chat: {str(e)}")
       return {
           "status": "error",
           "error": f"ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
       }


@app.get("/api/inventory")
def get_inventory(sort: str = None, main: str = None, sub1: str = None, sub2: str = None):
    try:
        response = supabase.from_('product_inventory').select("""
            id, value,
            product_info (
                main, sub1, sub2, sub3
            )
        """).execute()

        df_inventory = pd.DataFrame(response.data)

        # âœ… `id` ê°’ì„ ìœ ì§€í•˜ê³  ë¬¸ìì—´ë¡œ ë³€í™˜ (í”„ë¡ íŠ¸ì™€ ì¼ê´€ë˜ê²Œ ë§¤í•‘)
        df_inventory["id"] = df_inventory["id"].astype(str)

        # âœ… `product_info` ì»¬ëŸ¼ì—ì„œ í•„ìš”í•œ ê°’ ì¶”ì¶œ
        df_inventory["main"] = df_inventory["product_info"].apply(lambda x: x.get("main", None) if isinstance(x, dict) else None)
        df_inventory["sub1"] = df_inventory["product_info"].apply(lambda x: x.get("sub1", None) if isinstance(x, dict) else None)
        df_inventory["sub2"] = df_inventory["product_info"].apply(lambda x: x.get("sub2", None) if isinstance(x, dict) else None)
        df_inventory["sub3"] = df_inventory["product_info"].apply(lambda x: x.get("sub3", None) if isinstance(x, dict) else None)

        # âœ… ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±° (`product_info`ëŠ” ì›ë³¸ JSONì´ë¯€ë¡œ ì‚­ì œ)
        df_inventory = df_inventory.drop(columns=["product_info"])
        
        # âœ… í•„í„°ë§ ì ìš©
        if main:
            df_inventory = df_inventory[df_inventory["main"] == main]
        if sub1:
            df_inventory = df_inventory[df_inventory["sub1"] == sub1]
        if sub2:
            df_inventory = df_inventory[df_inventory["sub2"] == sub2]

        # âœ… ì •ë ¬ì„ í”„ë¡ íŠ¸ì—”ë“œì—ì„œë§Œ ë‹´ë‹¹ (ë°±ì—”ë“œëŠ” ì •ë ¬ X)
        if sort:
            df_inventory = df_inventory.sort_values(by=["value"], ascending=(sort == "asc"))

        return df_inventory.to_dict(orient="records")
    
    
    except Exception as e:
        print(f"âŒ Error fetching inventory: {e!s}")
        return {"error": str(e)}

# âœ… ì¹´í…Œê³ ë¦¬ í•„í„° ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/category_filters")
def get_category_filters(main: str = None, sub1: str = None, sub2: str = None):
    """
    ì„ íƒëœ main, sub1, sub2ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ëŠ¥í•œ sub1, sub2, sub3 ëª©ë¡ ë°˜í™˜
    """
    try:
        response = supabase.from_('product_info').select("main, sub1, sub2, sub3").execute()
        df = pd.DataFrame(response.data)

        filters = {}

        if main and sub1 and sub2:
            filters["sub3"] = sorted(df[(df["main"] == main) & (df["sub1"] == sub1) & (df["sub2"] == sub2)]["sub3"].dropna().unique().tolist())
        elif main and sub1:
            filters["sub2"] = sorted(df[(df["main"] == main) & (df["sub1"] == sub1)]["sub2"].dropna().unique().tolist())
        elif main:
            filters["sub1"] = sorted(df[df["main"] == main]["sub1"].dropna().unique().tolist())
        else:
            filters["main"] = sorted(df["main"].dropna().unique().tolist())

        return {"status": "success", "filters": filters}

    except Exception as e:
        return {"error": str(e)}


# âœ… ìµœì†Œ ì¬ê³  ê¸°ì¤€(ROP) ê³„ì‚° (ì„œë²„ ì‹¤í–‰ ì‹œ í•œ ë²ˆë§Œ ìˆ˜í–‰)
def compute_fixed_reorder_points():
    try:
        response = supabase.from_('monthly_sales').select("*").execute()
        df_sales = pd.DataFrame(response.data)

        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        infer_model = StackedLSTMModel().to(device)

        # ì €ì¥ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
        infer_model.load_state_dict(torch.load("weight/best_model.pth", map_location=device))
        pred = inference(infer_model, test_loader, device)
        # 1ï¸âƒ£ ğŸ”¹ Inverse Scaling ì ìš©
        for idx in range(len(pred)):
            if isinstance(scale_min_dict, dict):  # âœ… ë”•ì…”ë„ˆë¦¬ì¼ ê²½ìš°
                min_val = scale_min_dict.get(idx, 0)  # ê¸°ë³¸ê°’ 0
                max_val = scale_max_dict.get(idx, 1)  # ê¸°ë³¸ê°’ 1
            else:  # ë¦¬ìŠ¤íŠ¸ë‚˜ ë„˜íŒŒì´ ë°°ì—´ì¼ ê²½ìš°
                min_val = scale_min_dict[idx]
                max_val = scale_max_dict[idx]

            pred[idx] = pred[idx] * (max_val - min_val) + min_val

        # 2ï¸âƒ£ ğŸ”¹ ì˜ˆì¸¡ ë°ì´í„° í›„ì²˜ë¦¬ (ë°˜ì˜¬ë¦¼ ë° ì •ìˆ˜ ë³€í™˜)
        pred = np.round(pred, 0).astype(int)

        # 3ï¸âƒ£ ğŸ”¹ ìŒìˆ˜ê°’ì„ 0ìœ¼ë¡œ ë³€ê²½
        pred[pred < 0] = 0

        # 4ï¸âƒ£ ğŸ”¹ 1ì°¨ì› ë°°ì—´ì´ë©´ (N, 1)ë¡œ ë³€í™˜
        if pred.ndim == 1:
            pred = pred.reshape(-1, 1)

        if df_sales.empty:
            return {}

        # âœ… ì „ì²´ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ì›” í‰ê·  íŒë§¤ëŸ‰ ê³„ì‚°
        valid_dates = [col for col in df_sales.columns if re.match(r"\d{2}_m\d{2}", col)]
        df_sales["monthly_avg_sales"] = df_sales[valid_dates].mean(axis=1).fillna(0)

        # âœ… ì „ì²´ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ì¼ í‰ê·  íŒë§¤ëŸ‰ ê³„ì‚° -> ì—¬ê¸°ë¥¼ ëª¨ë¸ inference ê²°ê³¼
        df_sales["daily_avg_sales"] = pred

        # âœ… ìµœì†Œ ì¬ê³  ê¸°ì¤€(ROP) ê³„ì‚°: ì¼ í‰ê·  íŒë§¤ëŸ‰ì˜ 2ë°° + 10
        df_sales["reorder_point"] = (df_sales["daily_avg_sales"] * 2 + 10).fillna(10)

        # âœ… `id`ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ JSON ì§ë ¬í™” ì˜¤ë¥˜ ë°©ì§€
        df_sales["id"] = df_sales["id"].astype(str)

        # âœ… ìµœì†Œ ì¬ê³  ê¸°ì¤€ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥
        reorder_points = df_sales.set_index("id")["reorder_point"].to_dict()

        return reorder_points

    except Exception as e:
        print(f"âŒ ìµœì†Œ ì¬ê³  ê¸°ì¤€ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
        return {}

# âœ… ì„œë²„ ì‹¤í–‰ ì‹œ ìµœì´ˆ ê³„ì‚°í•˜ì—¬ ì €ì¥
FIXED_REORDER_POINTS = compute_fixed_reorder_points()

@app.get("/api/reorder_points")
def get_reorder_points(start: str, end: str):
    try:
        response = supabase.from_('monthly_sales').select("*").execute()
        df_sales = pd.DataFrame(response.data)

        if df_sales.empty:
            return {"error": "ğŸš¨ Supabaseì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}

        # âœ… ì„ íƒí•œ ê¸°ê°„ì˜ ì›”ë³„ íŒë§¤ëŸ‰ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        valid_dates = [col for col in df_sales.columns if re.match(r"\d{2}_m\d{2}", col)]
        selected_dates = [col for col in valid_dates if start <= col <= end]

        if not selected_dates:
            return {"error": "ğŸš¨ ì„ íƒí•œ ê¸°ê°„ì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

        # âœ… ì„ íƒí•œ ê¸°ê°„ì— ëŒ€í•œ ì›”/ì¼ í‰ê·  íŒë§¤ëŸ‰ ê³„ì‚°
        df_sales["monthly_avg_sales"] = df_sales[selected_dates].mean(axis=1).fillna(0)
        df_sales["daily_avg_sales"] = df_sales["monthly_avg_sales"] / 30

        # âœ… ìµœì†Œ ì¬ê³  ê¸°ì¤€(ROP)ì€ ì„œë²„ ì‹œì‘ ì‹œ ê³„ì‚°ëœ ê°’ ì‚¬ìš©
        df_sales["reorder_point"] = df_sales["id"].map(lambda x: FIXED_REORDER_POINTS.get(str(x), 10))

        # âœ… IDë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ JSON ì§ë ¬í™” ë¬¸ì œ ë°©ì§€
        df_sales["id"] = df_sales["id"].astype(str)

        # âœ… í•„ìš”í•œ ë°ì´í„°ë§Œ ë°˜í™˜
        reorder_data = df_sales[["id", "monthly_avg_sales", "daily_avg_sales", "reorder_point"]].to_dict(orient="records")

        return reorder_data

    except Exception as e:
        print(f"âŒ ROP ê°±ì‹  ì˜¤ë¥˜: {str(e)}")
        return {"error": str(e)}

active_connections = []
db_path = os.path.join(os.path.dirname(__file__), "../database/database.db")

def order_db():
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    cursor.execute(
        "SELECT name FROM sqlite_master WHERE type='table' AND name='auto_orders';"
    )
    table_exists = cursor.fetchone()

    # í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±
    if not table_exists:
        cursor.execute(
            """CREATE TABLE auto_orders (
                id INTEGER PRIMARY KEY,
                value INTEGER,
                is_orderable BOOLEAN
            )"""
        )
        conn.commit()
        print("âœ… í…Œì´ë¸” 'auto_orders'ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    conn.close()

order_db()

# âœ… ì£¼ë¬¸ ë°ì´í„° ëª¨ë¸
class OrderItem(BaseModel):
    id: int
    value: int
    is_orderable: bool

class OrderData(BaseModel):
    items: List[OrderItem]

# âœ… WebSocket í•¸ë“¤ëŸ¬ (í”„ë¡ íŠ¸ì—”ë“œì™€ ì‹¤ì‹œê°„ ì—°ê²°)
@app.websocket("/ws/auto_orders")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    active_connections.append(websocket)
    try:
        while True:
            await websocket.receive_text()  # í´ë¼ì´ì–¸íŠ¸ ë©”ì‹œì§€ ìˆ˜ì‹  (í•„ìš” ì—†ìœ¼ë©´ ì œê±° ê°€ëŠ¥)
    except WebSocketDisconnect:
        active_connections.remove(websocket)

# âœ… ì£¼ë¬¸ ë°ì´í„° ì €ì¥ + WebSocketìœ¼ë¡œ í”„ë¡ íŠ¸ì— ì „ì†¡
@app.post("/api/auto_orders")
async def save_auto_order(order_data: OrderData):
    if not order_data.items:
        raise HTTPException(status_code=400, detail="ì˜ëª»ëœ ë°ì´í„° í˜•ì‹ì…ë‹ˆë‹¤.")
    db_path = os.path.join(os.path.dirname(__file__), "../database/database.db")
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    for item in order_data.items:
        cursor.execute(
            """INSERT INTO auto_orders (id, value, is_orderable)
               VALUES (?, ?, ?)
               ON CONFLICT(id) DO UPDATE SET value = ?, is_orderable = ?""",
            (item.id, item.value, item.is_orderable, item.value, item.is_orderable),
        )

    conn.commit()
    conn.close()

    # âœ… ì£¼ë¬¸ ì™„ë£Œ ì´ë²¤íŠ¸ë¥¼ WebSocketì„ í†µí•´ í”„ë¡ íŠ¸ì— ì „ì†¡
    order_info = {"id": item.id, "value": item.value, "status": "âœ… ì£¼ë¬¸ ì™„ë£Œ"}
    for connection in active_connections:
        await connection.send_json(order_info)  # í”„ë¡ íŠ¸ì—”ë“œì— JSON ë°ì´í„° ì „ì†¡

    return {"status": "success", "message": "ìë™ ì£¼ë¬¸ ì™„ë£Œ ë¦¬ìŠ¤íŠ¸ ì €ì¥ë¨"}

# main
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=False) 
